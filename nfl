#!/bin/python3

import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


def NflScraper():
    url = 'https://www.pro-football-reference.com'
    year = 2021
    maxp = 200

    r = requests.get(url + '/years/' + str(year) + '/fantasy.htm')
    soup = BeautifulSoup(r.content, 'html.parser')
    parsed_table = soup.find_all('table')[0]

    df = []
    positions = ['RB', 'WR', 'QB']

    for i, row in enumerate(parsed_table.find_all('tr')[2:]):
        if i % 10 == 0:
            print(i, end=' ')
        if i >= maxp:
            print('\nComplete.')
            break

        try:
            dat = row.find('td', attrs={'data-stat': 'player'})
            name = dat.a.get_text()
            print(name)
            stub = dat.a.get('href')
            stub = stub[:-4] + '/gamelog/2021/'
            pos = row.find('td', attrs={'data-stat': 'fantasy_pos'}).get_text()

            tdf = pd.read_html(url + stub)[0]
            tdf.columns = tdf.columns.get_level_values(-1)
            tdf.drop(tdf.tail(1).index, inplace=True)
            if pos == 'RB':
                tdf = tdf.iloc[:, [7, 10, 11, 13]]
                df.append(tdf)
            if pos == 'WR':
                tdf = tdf.iloc[:, [7, 10, 11, 12, 14]]
                df.append(tdf)
            if pos == 'QB':
                tdf = tdf.iloc[:, [7, 10, 11, 13, 14, 15]]
                df.append(tdf)
            tdf['Name'] = name
            tdf['Position'] = pos

        except AttributeError:
            pass

    df = pd.concat(df)
    print(df)

    # noinspection PyTypeChecker
    df.to_csv('/Users/arslanamir/PycharmProjects/nfl.csv', index=False)

def FanduelScraper():
    sheet = pd.read_csv('/Users/arslanamir/PycharmProjects/nfl.csv')
    s = Service('/usr/local/bin/chromedriver')
    driver = webdriver.Chrome(service=s)
    url = 'https://sportsbook.fanduel.com/navigation/nfl'
    driver.get(url)
    games = WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, "More wagers")))
    for i in range(len(games)):
        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((games[i]))).click()
        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, "//*[contains(text(),'Passing Props')]")))\
            .click()
        props = WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.XPATH, "//*[contains(text(), 'Player')]")))
        for x in range(len(props)):
            name = props[0].find_element(By.XPATH, '//*[@id="root"]/div/div[2]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[3]/div[1]/div/div/div[1]/span').text
            print(name)
            parent_name = name.find_element(By.XPATH, './/parent::div')
            line_over = parent_name.find_element(By.XPATH, './/following-sibling::div/div/div/div/span')
            line_over_odds = line_over.find_element(By.XPATH, './/following-sibling::span').text
            line_under = parent_name.find_element(By.XPATH, './/following-sibling::div/div/div[2]/div/span')
            line_under_odds = line_under.find_element(By.XPATH, './/following-sibling::span').text
            print(line_over.text, line_over_odds, line_under.text, line_under_odds)
        driver.get(url)
        games = WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, "More wagers")))
