#!/bin/python3

import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException


def nba_scraper():
    url = 'https://www.basketball-reference.com/leagues/NBA_2022_totals.html#totals_stats::pts'
    maxp = 550
    r = requests.get(url)
    soup = BeautifulSoup(r.content, 'html.parser')
    parsed_table = soup.find_all('table')[-1]

    df = []

    for i, row in enumerate(parsed_table.find_all('tr')[1:]):
        if i % 10 == 0:
            print(i, end=' ')
        if i >= maxp:
            print('\nComplete.')
            break
        try:
            dat = row.find('td', attrs={'data-stat': 'player'})
            name = dat.a.get_text()
            print(name)
            stub = dat.a.get('href')
            stub = stub[:-5] + '/gamelog/2022'
            try:
                tdf = pd.read_html('https://www.basketball-reference.com' + stub)[-1]
                tdf.columns = tdf.columns.get_level_values(-1)
                tdf.drop(tdf.tail(1).index, inplace=True)

                tdf = tdf.iloc[:, [6, 10, 11, 13, 14, 16, 17, 19, 20, 21, 22, 23, 24, 25, 27]]
                tdf = tdf.drop(tdf[(tdf['FG'] == 'Inactive') | (tdf['FG'] == 'Did Not Play') |
                                   (tdf['FG'] == 'Did Not Dress') | (tdf['FG'] == 'Not With Team') |
                                   (tdf['FG'] == 'Player Suspended')].index)
                try:
                    tdf.drop(20, inplace=True)
                except KeyError:
                    pass
                df.append(tdf)
                tdf['Name'] = name
            except ValueError:
                pass

        except AttributeError:
            pass

    df = pd.concat(df)
    print(df)

    # noinspection PyTypeChecker
    df.to_csv('/Users/arslanamir/PycharmProjects/nba.csv', index=False)


def fanduel_scraper():
    bets, count = 0, 0
    picks = {}
    s = Service('/usr/local/bin/chromedriver')
    driver = webdriver.Chrome(service=s)
    url = 'https://sportsbook.fanduel.com/navigation/nba'
    driver.get(url)
    games = WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located
                                           ((By.PARTIAL_LINK_TEXT, "More wagers")))
    pages = ['Points', 'Rebounds', 'Assists', 'Threes']
    for page in pages:
        for i in range(len(games)):
            WebDriverWait(driver, 5).until(EC.element_to_be_clickable((games[i]))).click()
            try:
                WebDriverWait(driver, 5).until(EC.element_to_be_clickable
                                                ((By.XPATH, f"//*[contains(text(),'{page}')]"))).click()
            except TimeoutException:
                driver.get(url)
                games = WebDriverWait(driver, 5).until(
                    EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, "More wagers")))
                continue
            #props = WebDriverWait(driver, 5).until(
                #EC.visibility_of_element_located((By.XPATH, "//*[contains(text(), 'Player')]")))
            try:
                WebDriverWait(driver, 1).until(EC.element_to_be_clickable
                                               ((By.XPATH, "//*[contains(text(), 'Show more')]"))).click()
            except TimeoutException:
                pass
            length = driver.find_elements(By.XPATH, "//span[text()='UNDER']//following::div[1]/*")
            for y in range(1, len(length)+1):
                name = WebDriverWait(driver, 20).until(
                    EC.visibility_of_element_located
                    ((By.XPATH, f"//span[text()='UNDER']//following::div[1]/div[{y}]//span")))
                print(name.text, page)
                count += 1
                if page == 'Points':
                    cat = 'PTS'
                elif page == 'Rebounds':
                    cat = 'TRB'
                elif page == 'Assists':
                    cat = 'AST'
                else:
                    cat = '3P'
                try:
                    parent_name = name.find_element(By.XPATH, './/parent::div')
                    line_over = parent_name.find_element(By.XPATH, './/following-sibling::div/div/div/div/span')
                    line_over_odds = line_over.find_element(By.XPATH, './/following-sibling::span').text
                    line_under = parent_name.find_element(By.XPATH, './/following-sibling::div/div/div[2]/div/span')
                    line_under_odds = line_under.find_element(By.XPATH, './/following-sibling::span').text
                    print(line_over.text, line_over_odds, line_under.text, line_under_odds)
                    if stat_checker(name.text, cat) < float(line_over.text[2:]) - 1:
                        print(stat_checker(name.text, cat))
                        print('UNDER PLACED', line_under_odds, stat_checker(name.text, cat))
                        bets += 1
                        picks[name.text + cat] = name.text, 'U', float(line_under.text[2:]), float(line_under_odds), \
                                                 stat_checker(name.text, cat), cat
                    elif stat_checker(name.text, cat) > float(line_over.text[2:]) + 1:
                        print(stat_checker(name.text, cat))
                        print('OVER PLACED', line_over_odds)
                        bets += 1
                        picks[name.text + cat] = name.text, 'O', float(line_over.text[2:]), float(line_over_odds), \
                                                 stat_checker(name.text, cat), cat
                    else:
                        pass

                except NoSuchElementException:
                    print('Locked')
                    pass
                print('##############################################')

            driver.get(url)
            games = WebDriverWait(driver, 5).until(
                EC.presence_of_all_elements_located((By.PARTIAL_LINK_TEXT, "More wagers")))
    print(f'Bets placed: {bets} out of {count} possible')
    printout = pd.DataFrame.from_dict(picks).T
    printout.columns = ['Name', 'O/U', 'Strike', 'Odds', 'Median', 'Cat']
    # noinspection PyTypeChecker
    printout.to_csv('/Users/arslanamir/PycharmProjects/nba_bets.csv', index=False)


def stat_checker(name, cat):
    sheet = pd.read_csv('/Users/arslanamir/PycharmProjects/nba.csv')
    sheet = sheet[sheet['Name'] == f'{name}']
    return sheet[f'{cat}'].median()

